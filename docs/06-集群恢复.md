# 🔄 集群恢复指南

本文档详细介绍如何使用备份文件恢复 Kubernetes 集群，确保在灾难发生时能够快速恢复业务。

## 📋 恢复概述

### 什么是集群恢复？

集群恢复是指使用之前创建的备份文件，将 Kubernetes 集群恢复到备份时点状态的过程。这包括：

- **🗄️ Etcd 数据恢复**: 恢复集群状态和配置信息
- **🔐 证书文件恢复**: 恢复 TLS 证书和密钥
- **⚙️ 配置文件恢复**: 恢复 kubelet 和组件配置
- **🔧 服务重建**: 重新启动和配置集群服务

### 恢复场景

| 场景类型 | 描述 | 恢复策略 |
|:---------|:-----|:---------|
| **硬件故障** | 服务器硬件损坏 | 在新硬件上完全恢复 |
| **误操作** | 意外删除关键资源 | 选择性恢复特定数据 |
| **灾难性故障** | 整个数据中心不可用 | 异地环境完全恢复 |
| **数据损坏** | Etcd 数据损坏 | 从最近备份恢复 |
| **升级回滚** | 升级失败需要回滚 | 恢复到升级前状态 |

## ⚠️ 恢复前准备

### 🚨 重要约定

1. **节点状态要求**
   - 目标节点必须是"干净"的状态
   - 如果已安装 Docker 或 kubelet，建议先卸载
   - 恢复过程会重新安装必要组件

2. **备份文件要求**
   - 使用 `cluster-backup` 目录中的备份文件
   - 确保备份文件完整且未损坏
   - 验证备份文件的时间戳和版本

3. **网络要求**
   - 确保所有节点网络连通
   - 验证 Ansible 控制节点可以访问所有目标节点
   - 检查防火墙和安全组设置

### 📋 恢复前检查清单

- [ ] 确认备份文件完整性
- [ ] 验证目标节点状态
- [ ] 检查网络连通性
- [ ] 准备主机清单文件
- [ ] 确认恢复时间窗口
- [ ] 通知相关人员
- [ ] 准备应急方案

## 🔍 验证备份文件

### 检查备份完整性

```bash
# 进入备份目录
cd cluster-backup/

# 检查备份文件是否存在
ls -la

# 验证备份文件完整性
for file in *.tar.gz; do
    echo "检查文件: $file"
    tar -tzf "$file" > /dev/null && echo "✅ 完整" || echo "❌ 损坏"
done

# 检查 Etcd 快照
if ls etcd-snapshot-*.db 1> /dev/null 2>&1; then
    echo "✅ Etcd 快照文件存在"
else
    echo "❌ 缺少 Etcd 快照文件"
fi

# 查看备份信息
cat backup-info.txt
```

### 备份文件清单验证

```bash
# 创建备份验证脚本
cat > verify-restore-ready.sh << 'EOF'
#!/bin/bash

echo "🔍 验证恢复准备情况..."

# 检查备份目录
if [ ! -d "cluster-backup" ]; then
    echo "❌ cluster-backup 目录不存在"
    exit 1
fi

cd cluster-backup/

# 检查备份信息文件
if [ ! -f "backup-info.txt" ]; then
    echo "❌ 缺少备份信息文件"
    exit 1
fi

# 检查 Etcd 快照
ETCD_SNAPSHOT=$(ls etcd-snapshot-*.db 2>/dev/null | head -1)
if [ -z "$ETCD_SNAPSHOT" ]; then
    echo "❌ 缺少 Etcd 快照文件"
    exit 1
fi

# 检查节点备份文件数量
NODE_BACKUPS=$(ls *-kubernetes.orig.*.tar.gz 2>/dev/null | wc -l)
if [ "$NODE_BACKUPS" -eq 0 ]; then
    echo "❌ 没有找到节点备份文件"
    exit 1
fi

echo "✅ 备份文件验证通过"
echo "📊 发现 $NODE_BACKUPS 个节点备份文件"
echo "📊 Etcd 快照: $ETCD_SNAPSHOT"

# 显示备份信息
echo ""
echo "📋 备份信息:"
cat backup-info.txt

echo ""
echo "🎉 恢复准备检查完成！"
EOF

chmod +x verify-restore-ready.sh
./verify-restore-ready.sh
```

## 🚀 执行集群恢复

### 方法一：基本配置恢复

适用于使用默认配置部署的集群：

```bash
# 执行集群恢复
ansible-playbook -i example/hosts.m-master.ip.ini 94-restore-cluster.yml
```

### 方法二：高级配置恢复

适用于使用自定义配置部署的集群：

```bash
# 使用自定义配置执行恢复
ansible-playbook -i example/hosts.m-master.ip.ini -e @example/variables.yaml 94-restore-cluster.yml
```

> **重要提醒**: 恢复时的配置参数必须与备份时保持一致，特别是 `example/variables.yaml` 文件中的设置。

### 监控恢复过程

```bash
# 监控 Ansible 执行过程
tail -f /tmp/ansible.log

# 在另一个终端监控节点状态（恢复完成后）
watch kubectl get nodes

# 监控 Pod 状态（恢复完成后）
watch kubectl get pods --all-namespaces
```

## 🔄 恢复流程详解

### 恢复步骤说明

1. **🧹 环境清理**
   - 停止现有的 Kubernetes 服务
   - 清理旧的配置文件和数据
   - 卸载不兼容的组件

2. **📦 组件安装**
   - 重新安装 Docker 或 containerd
   - 安装 kubelet、kubeadm、kubectl
   - 配置必要的系统参数

3. **📁 文件恢复**
   - 恢复 Kubernetes 配置文件
   - 恢复证书和密钥文件
   - 恢复 kubelet 配置

4. **🗄️ Etcd 数据恢复**
   - 恢复 Etcd 数据快照
   - 重新初始化 Etcd 集群
   - 验证数据完整性

5. **🔄 服务启动**
   - 启动 kubelet 服务
   - 启动控制平面组件
   - 启动工作节点组件

6. **🔍 状态验证**
   - 验证集群连通性
   - 检查所有节点状态
   - 验证核心服务功能

### 恢复时间估算

| 集群规模 | 预估时间 | 说明 |
|:---------|:---------|:-----|
| 小型集群 (3-5 节点) | 45-90 分钟 | 包含组件重装时间 |
| 中型集群 (6-20 节点) | 1.5-3 小时 | 取决于数据量大小 |
| 大型集群 (20+ 节点) | 3-6 小时 | 可能需要分批恢复 |

## ✅ 恢复后验证

### 1. 基础功能验证

```bash
# 等待所有节点就绪
kubectl get nodes

# 检查系统 Pod 状态
kubectl get pods -n kube-system

# 验证集群信息
kubectl cluster-info

# 检查集群组件状态
kubectl get componentstatuses
```

### 2. 详细功能测试

```bash
# 测试 API Server 响应
kubectl version

# 测试命名空间列表
kubectl get namespaces

# 测试服务发现
kubectl get services --all-namespaces

# 测试存储类
kubectl get storageclass

# 测试网络策略（如果启用）
kubectl get networkpolicies --all-namespaces
```

### 3. Etcd 集群验证

```bash
# 检查 Etcd 集群健康状态
kubectl -n kube-system exec -it etcd-$(hostname) -- etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  endpoint health

# 检查 Etcd 成员列表
kubectl -n kube-system exec -it etcd-$(hostname) -- etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  member list
```

### 4. 应用工作负载测试

```bash
# 创建测试 Pod
kubectl run test-pod --image=nginx --restart=Never

# 检查 Pod 状态
kubectl get pod test-pod -o wide

# 测试 Pod 内网络
kubectl exec test-pod -- nslookup kubernetes.default

# 清理测试 Pod
kubectl delete pod test-pod
```

## 🛠️ 故障排查

### 常见问题及解决方案

#### 1. 恢复过程失败

**症状**: Ansible 执行过程中报错

**解决方案**:
```bash
# 检查节点连通性
ansible -i example/hosts.m-master.ip.ini all -m ping

# 检查节点状态
ansible -i example/hosts.m-master.ip.ini all -m command -a "systemctl status kubelet"

# 重新执行恢复（幂等操作）
ansible-playbook -i example/hosts.m-master.ip.ini 94-restore-cluster.yml
```

#### 2. 节点无法加入集群

**症状**: Worker 节点显示 NotReady

**解决方案**:
```bash
# 检查 kubelet 日志
sudo journalctl -u kubelet -f

# 检查证书有效性
sudo openssl x509 -in /etc/kubernetes/pki/ca.crt -noout -text

# 重启 kubelet 服务
sudo systemctl restart kubelet
```

#### 3. Etcd 集群异常

**症状**: Etcd 无法启动或数据不一致

**解决方案**:
```bash
# 检查 Etcd 数据目录
sudo ls -la /var/lib/etcd/

# 检查 Etcd 日志
sudo journalctl -u etcd -f

# 重新恢复 Etcd 数据
# 注意：这会重新执行整个恢复流程
ansible-playbook -i example/hosts.m-master.ip.ini 94-restore-cluster.yml
```

#### 4. 网络插件问题

**症状**: Pod 网络不通

**解决方案**:
```bash
# 重新部署网络插件
ansible-playbook -i example/hosts.m-master.ip.ini 21-network-plugin.yml

# 检查网络插件 Pod 状态
kubectl get pods -n kube-system | grep -E "(calico|flannel)"

# 重启网络相关 Pod
kubectl delete pods -n kube-system -l k8s-app=calico-node
```

## 📋 恢复后任务

### 1. 应用恢复

恢复集群基础设施后，还需要恢复应用工作负载：

```bash
# 检查 PV/PVC 状态
kubectl get pv,pvc --all-namespaces

# 重新部署应用（如需要）
kubectl apply -f /path/to/application-manifests/

# 检查应用状态
kubectl get deployments --all-namespaces
kubectl get services --all-namespaces
```

### 2. 数据验证

```bash
# 验证关键应用数据
# 这需要根据具体应用进行定制化检查

# 检查配置映射和密钥
kubectl get configmaps --all-namespaces
kubectl get secrets --all-namespaces

# 验证 RBAC 配置
kubectl get roles,rolebindings --all-namespaces
```

### 3. 监控和告警

```bash
# 重新启用监控服务（如 Prometheus）
kubectl get pods -n monitoring

# 检查日志收集服务
kubectl get pods -n logging

# 验证告警规则
kubectl get prometheusrules --all-namespaces
```

## 🎯 最佳实践

### 1. 定期演练

```bash
# 创建恢复演练脚本
cat > disaster-recovery-drill.sh << 'EOF'
#!/bin/bash

echo "🎭 开始灾难恢复演练..."

# 记录开始时间
START_TIME=$(date)
echo "演练开始时间: $START_TIME"

# 执行恢复流程
echo "1. 验证备份文件..."
./verify-restore-ready.sh

echo "2. 执行集群恢复..."
ansible-playbook -i example/hosts.m-master.ip.ini 94-restore-cluster.yml

echo "3. 验证恢复结果..."
kubectl get nodes
kubectl get pods --all-namespaces

# 记录完成时间
END_TIME=$(date)
echo "演练完成时间: $END_TIME"

echo "🎉 灾难恢复演练完成！"
EOF

chmod +x disaster-recovery-drill.sh
```

### 2. 文档更新

```bash
# 记录恢复操作
cat > recovery-log.md << EOF
# 集群恢复记录

## 基本信息
- 恢复时间: $(date)
- 操作人员: $(whoami)
- 备份时间: $(cat cluster-backup/backup-info.txt | grep "备份时间" || echo "未知")
- 集群版本: $(kubectl version --short 2>/dev/null || echo "未知")

## 恢复原因
<!-- 描述恢复的原因和背景 -->

## 恢复过程
<!-- 记录恢复步骤和遇到的问题 -->

## 验证结果
<!-- 记录验证步骤和结果 -->

## 经验教训
<!-- 记录改进建议和经验总结 -->
EOF
```

## 📚 相关文档

- [💾 集群备份](05-集群备份.md) - 了解如何创建备份
- [⬆️ 集群升级](04-集群升级.md) - 升级失败时的恢复
- [🔧 节点管理](02-节点管理.md) - 节点相关操作
- [🚀 集群安装](01-集群安装.md) - 了解集群架构

## 📞 获取帮助

如果在集群恢复过程中遇到问题：

1. 📖 查看详细的 Ansible 执行日志
2. 🔍 检查 Kubernetes 组件日志
3. 📋 确认备份文件的完整性和兼容性
4. 🐛 [报告问题](https://github.com/TimeBye/Kubernetes/issues) 获取技术支持

---

**重要提示**:
- 🚨 恢复操作有风险，建议先在测试环境验证
- 💾 确保备份文件的完整性和时效性
- 📅 定期进行恢复演练，验证恢复流程的有效性
- 👥 建议多人协作，确保恢复过程的安全性
- 📋 详细记录恢复过程，为后续改进提供参考