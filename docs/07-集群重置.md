# 🔄 集群重置指南

本文档详细介绍如何安全地重置 Kubernetes 集群，清除所有集群数据和配置，恢复到初始状态。

## 📋 重置概述

### 什么是集群重置？

集群重置是指完全清除 Kubernetes 集群的所有数据、配置和状态，将所有节点恢复到安装前的干净状态。这个操作会：

- **🗑️ 删除所有 Pod 和容器**
- **🔐 清除证书和密钥**
- **⚙️ 删除配置文件**
- **🗄️ 清空 Etcd 数据**
- **🔧 停止相关服务**
- **📁 清理数据目录**

### 重置使用场景

| 场景 | 描述 | 建议操作 |
|:-----|:-----|:---------|
| **部署失败** | 初始部署过程出错 | 重置后重新部署 |
| **测试完成** | 测试环境不再需要 | 重置释放资源 |
| **配置错误** | 集群配置严重错误 | 重置后重新配置 |
| **环境迁移** | 需要更换集群环境 | 备份→重置→重建 |
| **安全事件** | 集群遭受安全威胁 | 重置后加强安全 |

## ⚠️ 重置前重要提醒

### 🚨 数据丢失警告

> **⚠️ 危险操作**: 集群重置会**永久删除**所有数据，包括：
> - 所有 Kubernetes 资源（Pod、Service、Deployment 等）
> - 用户数据和应用状态
> - 集群配置和证书
> - Etcd 存储的所有数据
> - 网络配置和策略

### 📋 重置前检查清单

- [ ] **数据备份**: 确认重要数据已备份
- [ ] **业务确认**: 确认所有业务可以中断
- [ ] **权限验证**: 确认具有操作权限
- [ ] **时间窗口**: 选择合适的维护时间
- [ ] **通知相关人员**: 提前通知使用集群的团队
- [ ] **清理计划**: 制定重置后的清理计划

### 💾 建议的重置前备份

```bash
# 1. 备份重要应用配置
kubectl get all --all-namespaces -o yaml > cluster-resources-backup.yaml

# 2. 备份自定义资源
kubectl get crd -o yaml > custom-resources-backup.yaml

# 3. 备份 ConfigMaps 和 Secrets
kubectl get configmaps --all-namespaces -o yaml > configmaps-backup.yaml
kubectl get secrets --all-namespaces -o yaml > secrets-backup.yaml

# 4. 执行完整集群备份
ansible-playbook -i example/hosts.m-master.ip.ini 93-backup-cluster.yml

# 5. 记录当前集群状态
kubectl get nodes -o wide > cluster-status-backup.txt
kubectl get pods --all-namespaces > pods-status-backup.txt
```

## 🚀 执行集群重置

### 方法一：基本配置重置

适用于使用默认配置部署的集群：

```bash
# 执行集群重置
ansible-playbook -i example/hosts.m-master.ip.ini 99-reset-cluster.yml
```

### 方法二：高级配置重置

适用于使用自定义配置部署的集群：

```bash
# 使用自定义配置执行重置
ansible-playbook -i example/hosts.m-master.ip.ini -e @example/variables.yaml 99-reset-cluster.yml
```

> **重要说明**: 
> - 如果集群部署时使用了 `example/variables.yaml` 文件，重置时也必须使用相同的配置参数
> - 当 `example/hosts.m-master.ip.ini` 与 `example/variables.yaml` 变量冲突时，`example/variables.yaml` 中的变量值优先级最高

### 交互式重置确认

```bash
# 创建安全重置脚本
cat > safe-reset.sh << 'EOF'
#!/bin/bash

set -e

echo "🚨 Kubernetes 集群重置工具"
echo "================================"
echo ""
echo "⚠️  警告: 此操作将完全删除集群的所有数据！"
echo "📋 包括: Pod、Service、配置、证书、Etcd数据等"
echo ""

# 显示当前集群状态
echo "📊 当前集群状态:"
kubectl get nodes 2>/dev/null || echo "无法连接到集群"
echo ""

# 二次确认
read -p "🤔 您确定要重置集群吗？请输入 'YES' 确认: " CONFIRM
if [ "$CONFIRM" != "YES" ]; then
    echo "❌ 操作已取消"
    exit 1
fi

echo ""
read -p "🔄 最后确认：这将删除所有数据，请再次输入 'DELETE' 确认: " FINAL_CONFIRM
if [ "$FINAL_CONFIRM" != "DELETE" ]; then
    echo "❌ 操作已取消"
    exit 1
fi

echo ""
echo "🚀 开始执行集群重置..."

# 检查配置文件
if [ -f "example/variables.yaml" ]; then
    echo "📋 使用高级配置执行重置"
    ansible-playbook -i example/hosts.m-master.ip.ini -e @example/variables.yaml 99-reset-cluster.yml
else
    echo "📋 使用基本配置执行重置"
    ansible-playbook -i example/hosts.m-master.ip.ini 99-reset-cluster.yml
fi

if [ $? -eq 0 ]; then
    echo ""
    echo "✅ 集群重置完成！"
    echo "💡 建议重启所有节点以确保完全清理"
else
    echo ""
    echo "❌ 集群重置失败，请检查日志"
fi
EOF

chmod +x safe-reset.sh
./safe-reset.sh
```

## 🔄 重置流程详解

### 重置步骤说明

1. **🛑 停止服务**
   - 停止 kubelet 服务
   - 停止容器运行时服务
   - 停止其他相关服务

2. **🗑️ 清理容器**
   - 删除所有运行中的容器
   - 清理容器镜像（可选）
   - 清空容器数据目录

3. **📁 删除配置文件**
   - 删除 `/etc/kubernetes/` 目录
   - 删除 kubelet 配置文件
   - 清理证书和密钥文件

4. **🗄️ 清理数据目录**
   - 清空 Etcd 数据目录
   - 清理 kubelet 数据目录
   - 删除网络配置文件

5. **🔧 系统清理**
   - 重置 iptables 规则
   - 清理网络命名空间
   - 恢复系统设置

6. **🔄 服务重置**
   - 重置 systemd 服务状态
   - 清理服务配置文件
   - 恢复默认设置

### 监控重置过程

```bash
# 监控 Ansible 执行过程
tail -f /tmp/ansible.log

# 检查节点状态（重置完成后应该无法连接）
kubectl get nodes

# 检查进程状态
ps aux | grep -E "(kube|etcd|docker|containerd)"

# 检查网络状态
ip route show
iptables -L
```

## ✅ 重置后验证

### 1. 验证服务状态

```bash
# 检查 Kubernetes 相关服务状态
systemctl status kubelet || echo "kubelet 已停止"
systemctl status docker || echo "docker 已停止"
systemctl status containerd || echo "containerd 已停止"

# 检查相关进程
ps aux | grep -E "(kube|etcd)" | grep -v grep || echo "没有 Kubernetes 进程运行"
```

### 2. 验证文件清理

```bash
# 检查配置目录
ls -la /etc/kubernetes/ 2>/dev/null || echo "✅ Kubernetes 配置目录已删除"

# 检查数据目录
ls -la /var/lib/kubelet/ 2>/dev/null || echo "✅ kubelet 数据目录已删除"
ls -la /var/lib/etcd/ 2>/dev/null || echo "✅ etcd 数据目录已删除"

# 检查证书目录
ls -la /etc/kubernetes/pki/ 2>/dev/null || echo "✅ 证书目录已删除"
```

### 3. 验证网络清理

```bash
# 检查容器网络
docker network ls 2>/dev/null | grep -v "bridge\|host\|none" || echo "✅ 容器网络已清理"

# 检查网络接口
ip link show | grep -E "(cni|flannel|calico)" || echo "✅ CNI 网络接口已清理"

# 检查 iptables 规则
iptables -L | grep -E "(KUBE|CNI)" || echo "✅ Kubernetes iptables 规则已清理"
```

### 4. 验证容器清理

```bash
# 检查运行中的容器
docker ps -a 2>/dev/null | grep -E "(kube|etcd|pause)" || echo "✅ Kubernetes 容器已清理"

# 或使用 crictl（containerd）
crictl ps -a 2>/dev/null | grep -E "(kube|etcd|pause)" || echo "✅ Kubernetes 容器已清理"
```

## 🧹 重置后清理

### 系统重启建议

```bash
# 创建重启脚本
cat > post-reset-reboot.sh << 'EOF'
#!/bin/bash

echo "🔄 集群重置后清理工具"
echo "===================="

# 显示重置验证结果
echo "📋 验证重置结果..."
echo ""

# 检查服务状态
echo "🔍 检查服务状态:"
systemctl is-active kubelet && echo "⚠️ kubelet 仍在运行" || echo "✅ kubelet 已停止"
systemctl is-active docker && echo "⚠️ docker 仍在运行" || echo "✅ docker 已停止"

# 检查进程
echo ""
echo "🔍 检查 Kubernetes 进程:"
K8S_PROCESSES=$(ps aux | grep -E "(kube|etcd)" | grep -v grep | wc -l)
if [ "$K8S_PROCESSES" -gt 0 ]; then
    echo "⚠️ 发现 $K8S_PROCESSES 个 Kubernetes 相关进程"
    ps aux | grep -E "(kube|etcd)" | grep -v grep
else
    echo "✅ 没有 Kubernetes 相关进程运行"
fi

echo ""
echo "💡 建议重启所有节点以确保完全清理残留文件"
echo ""

read -p "🔄 是否现在重启当前节点？(y/N): " REBOOT_CONFIRM
if [ "$REBOOT_CONFIRM" = "y" ] || [ "$REBOOT_CONFIRM" = "Y" ]; then
    echo "🔄 正在重启节点..."
    sudo shutdown -r now
else
    echo "ℹ️ 请稍后手动重启节点"
fi
EOF

chmod +x post-reset-reboot.sh
```

### 手动清理脚本

```bash
# 创建手动清理脚本（用于处理残留文件）
cat > manual-cleanup.sh << 'EOF'
#!/bin/bash

echo "🧹 手动清理残留文件"
echo "=================="

# 清理可能的残留目录
echo "🗑️ 清理残留目录..."
sudo rm -rf /var/lib/kubelet/
sudo rm -rf /var/lib/etcd/
sudo rm -rf /etc/kubernetes/
sudo rm -rf /var/lib/cni/
sudo rm -rf /etc/cni/
sudo rm -rf /opt/cni/

# 清理容器数据
echo "🗑️ 清理容器数据..."
sudo rm -rf /var/lib/docker/containers/
sudo rm -rf /var/lib/containerd/

# 清理网络配置
echo "🔗 重置网络配置..."
sudo iptables -F
sudo iptables -t nat -F
sudo iptables -t mangle -F
sudo iptables -X

# 清理 systemd 服务文件
echo "🔧 清理服务文件..."
sudo rm -f /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
sudo systemctl daemon-reload

echo "✅ 手动清理完成"
echo "💡 建议重启系统以确保完全清理"
EOF

chmod +x manual-cleanup.sh
```

## 🛠️ 故障排查

### 重置失败问题

#### 1. Ansible 执行失败

**症状**: 重置过程中 Ansible 报错

**解决方案**:
```bash
# 检查网络连接
ansible -i example/hosts.m-master.ip.ini all -m ping

# 检查节点状态
ansible -i example/hosts.m-master.ip.ini all -m command -a "uptime"

# 手动强制重置（谨慎使用）
ansible -i example/hosts.m-master.ip.ini all -m shell -a "kubeadm reset --force"
```

#### 2. 服务无法停止

**症状**: kubelet 或其他服务无法停止

**解决方案**:
```bash
# 强制停止服务
sudo systemctl stop kubelet --force
sudo systemctl stop docker --force
sudo systemctl stop containerd --force

# 杀死相关进程
sudo pkill -f kubelet
sudo pkill -f dockerd
sudo pkill -f containerd
```

#### 3. 文件删除失败

**症状**: 配置文件或数据目录无法删除

**解决方案**:
```bash
# 检查文件占用
sudo lsof | grep -E "(kubernetes|etcd|kubelet)"

# 强制解除挂载
sudo umount -f /var/lib/kubelet/pods/*/volumes/kubernetes.io~secret/*
sudo umount -f /var/lib/kubelet/pods/*/volumes/kubernetes.io~configmap/*

# 强制删除文件
sudo rm -rf /etc/kubernetes/
sudo rm -rf /var/lib/kubelet/
sudo rm -rf /var/lib/etcd/
```

## 📋 重置后重建集群

### 重新部署准备

重置完成后，如果需要重新部署集群：

```bash
# 1. 重启所有节点（推荐）
sudo reboot

# 2. 验证节点状态
uptime
free -h
df -h

# 3. 检查网络连接
ansible -i example/hosts.m-master.ip.ini all -m ping

# 4. 重新部署集群
ansible-playbook -i example/hosts.m-master.ip.ini 90-init-cluster.yml
```

### 配置文件更新

```bash
# 如果需要修改配置，编辑相关文件
vim example/hosts.m-master.ip.ini
vim example/variables.yaml

# 确保时间同步
ansible -i example/hosts.m-master.ip.ini all -m command -a "chrony sources -v"
```

## 📚 相关文档

- [🚀 集群安装](01-集群安装.md) - 重置后重新部署
- [💾 集群备份](05-集群备份.md) - 重置前的数据备份
- [🔄 集群恢复](06-集群恢复.md) - 从备份恢复集群
- [📋 安装须知](00-安装须知.md) - 了解系统要求

## 📞 获取帮助

如果在集群重置过程中遇到问题：

1. 📖 查看详细的 Ansible 执行日志
2. 🔍 检查系统日志和进程状态
3. 🛠️ 尝试手动清理脚本
4. 🐛 [报告问题](https://github.com/TimeBye/Kubernetes/issues) 获取技术支持

---

**重要提示**:
- 🚨 重置操作不可逆，务必确认后再执行
- 💾 重置前务必备份重要数据
- 🔄 重置后建议重启所有节点
- 📋 记录重置原因和过程，用于后续改进
- 👥 重要环境建议多人确认后操作